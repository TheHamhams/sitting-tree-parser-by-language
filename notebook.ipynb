{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for the folder for the jsonl files\n",
    "folder_name = 'folder name'\n",
    "\n",
    "# choose a language to parse from the dataset\n",
    "df_language = 'coding language'\n",
    "\n",
    "# use your huggingface username and the desired repo name\n",
    "hf_username = 'username'\n",
    "hf_repo_name = 'repo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install git-lfs # git extension\n",
    "!pip install transformers # transformers for viewing codeparrot dataset\n",
    "!pip install datasets # huggingface datasets\n",
    "!pip install function-parser # ncooper's function_parser -> parse github repos into docstrings and function code pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/tree-sitter/tree-sitter-python.git # tree-sitter-python for building language grammar for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm to create a progress bar for the parsing loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tree sitter and function parser imports\n",
    "import function_parser\n",
    "from tree_sitter import Language\n",
    "\n",
    "from function_parser.language_data import LANGUAGE_METADATA\n",
    "from function_parser.process import DataProcessor\n",
    "from tree_sitter import Language\n",
    "\n",
    "# transformers and datasets imports\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import list_datasets\n",
    "from huggingface_hub import notebook_login, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve path to function_parser\n",
    "parser_path = function_parser.__path__[0]\n",
    "parser_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the tree-siter-py to function_parser PATH\n",
    "Language.build_library(\n",
    "    # Store the library in the directory\n",
    "    f'{parser_path}/tree-sitter-languages.so',\n",
    "    # Include one or more languages\n",
    "    [\n",
    "        'tree-sitter-python'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the language param and build grammar for that language\n",
    "language = \"python\"\n",
    "DataProcessor.PARSER.set_language(\n",
    "    Language(os.path.join(function_parser.__path__[0], \"tree-sitter-languages.so\"), language)\n",
    ")\n",
    "processor = DataProcessor(\n",
    "    language=language, language_parser=LANGUAGE_METADATA[language][\"language_parser\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository</th>\n",
       "      <th>stars</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kavex/GameDev-Resources</td>\n",
       "      <td>1129</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pristineio/webrtc-mirror</td>\n",
       "      <td>152</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emacs-lsp/lsp-ui</td>\n",
       "      <td>635</td>\n",
       "      <td>Emacs Lisp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bwar/CJsonObject</td>\n",
       "      <td>328</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>defshine/cleanblog</td>\n",
       "      <td>108</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 repository  stars    language\n",
       "0   Kavex/GameDev-Resources   1129        None\n",
       "1  pristineio/webrtc-mirror    152         C++\n",
       "2          emacs-lsp/lsp-ui    635  Emacs Lisp\n",
       "3          Bwar/CJsonObject    328         C++\n",
       "4        defshine/cleanblog    108  JavaScript"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the full pile dataframe\n",
    "url = 'https://raw.githubusercontent.com/EleutherAI/github-downloader/master/github_repositories.csv'\n",
    "\n",
    "col_names = ['repository', 'stars', 'language']\n",
    "\n",
    "df_full = pd.read_csv(url, names=col_names)\n",
    "\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df only containing repositories in the desired language\n",
    "df = df_full[df_full.language == df_language]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6229\n"
     ]
    }
   ],
   "source": [
    "# create an array for repo names\n",
    "repos = df.repository.values\n",
    "\n",
    "# inspect the number or rows in repo\n",
    "total = repos.shape[0]\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for the jsonl files\n",
    "dir = os.getcwd()\n",
    "os.chdir(dir)\n",
    "os.mkdir(folder_name)\n",
    "os.chdir(dir)\n",
    "os.chdir(folder_name)\n",
    "json_path = os.getcwd()\n",
    "os.chdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the repos array and create a json file for each newly created dataframe\n",
    "for repo in tqdm(repos):\n",
    "  dependee = repo\n",
    "  definitions = processor.process_dee(dependee, ext=LANGUAGE_METADATA[language][\"ext\"])\n",
    "  pile = pd.DataFrame(definitions)\n",
    "  if pile.shape[0] > 0:\n",
    "    pile.dropna(subset=['docstring'], inplace=True)\n",
    "  if pile.shape[0] > 0:\n",
    "    pile = pile[pile.docstring != '']\n",
    "  if pile.shape[0] > 0:\n",
    "    pile.to_json(f\"{json_path}/{'__'.join(repo.split('/'))}.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset using all of the jsonl files\n",
    "the_pile_parsed = load_dataset(\"json\", data_files=f\"{json_path}/*.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_pile_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = list_datasets()\n",
    "print(f\"Number of datasets on Hub: {len(all_datasets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% train, 10% test + validation\n",
    "train_testvalid = the_pile_parsed.train_test_split(test_size=0.1)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to huggingface\n",
    "!pip install ipywidgets\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the huggingface repo\n",
    "repo = create_repo(name=hf_repo_name, repo_type=\"dataset\")\n",
    "repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#push the dataset to huggingface\n",
    "train_test_valid_dataset.push_to_hub(repo_id=f'{hf_username}/{hf_repo_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowload the newly pushed dataset\n",
    "read = load_dataset(f'{hf_username}/{hf_repo_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that dataset was pused correctly\n",
    "read"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6fd4c5a980a3528df50d727762bcd3be255b4e8e5d5700dbec320c90c6f2c3a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
